{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMzW/39djGcsz9BNW8P6RYm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chminPark/ml-python/blob/master/%EC%8B%A4%EC%8A%B5_%EB%B0%98%EB%8F%84%EC%B2%B4%EC%9D%B4%EB%AF%B8%EC%A7%80%EB%B6%84%EB%A5%98_CNN_%5B%EC%A1%B0%EA%B5%90%EC%9A%A9%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1nOUWByW6ifnd_P-Jgd4mbNkAT-jGzpWC\n",
        "!mkdir 'semicon'\n",
        "!unzip -q semiconductor_dataset.zip -d semicon\n",
        "!rm semiconductor_dataset.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvM0McnQ8Mx8",
        "outputId": "4c22d0f6-f68e-4a6c-f78f-f92688dffb74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gdown/cli.py:127: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1nOUWByW6ifnd_P-Jgd4mbNkAT-jGzpWC\n",
            "To: /content/semiconductor_dataset.zip\n",
            "100% 10.3M/10.3M [00:00<00:00, 58.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import os\n",
        "import pandas as pd\n",
        "\"\"\" \n",
        "Args:    \n",
        "  directory_string: 이미지가 저장되어 있는 폴더 Path\n",
        "  output_csv_name: csv 파일 이름\n",
        "Returns:\n",
        "  csv file \n",
        "\"\"\"\n",
        "def build_csv(directory_string, output_csv_name):\n",
        "    \n",
        "    directory = directory_string\n",
        "    class_list = os.listdir(directory) \n",
        "    class_list.sort() \n",
        "\n",
        "    \n",
        "    with open(output_csv_name, 'w', newline='') as csvfile:\n",
        "        \n",
        "        ### 실습 : CSV 파일 Object 생성 \n",
        "        writer = csv.writer(csvfile, delimiter=',')\n",
        "        ############################################\n",
        "        writer.writerow(['file_name', 'file_path', 'class_name', 'class_index']) # CSV의 column 이름을 지정\n",
        "\n",
        "        ###### 각 folder에 들어가서 각 이미지의 이름을 가져옴\n",
        "        for class_name in class_list:\n",
        "          class_path = os.path.join(directory, class_name)  \n",
        "          file_list = os.listdir(class_path) # 해당 파일 내부의 이미지를 확보\n",
        "          for file_name in file_list:\n",
        "              file_path = os.path.join(directory, class_name, file_name) #concatenate class folder dir, class name and file name\n",
        "              writer.writerow([file_name, file_path, class_name, class_name.split(\"_\")[1]]) #write the file path and class name to the csv file\n",
        "        #############################\n",
        "            \n",
        "    return\n",
        "\n",
        "train_folder = os.path.join(os.getcwd(), 'semicon')\n",
        "build_csv(train_folder, 'train.csv')\n",
        "train_df = pd.read_csv('train.csv')\n"
      ],
      "metadata": {
        "id": "5dXEooJlWsRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom데이터를 활용하기 위한 Dataset을 선언\n",
        "- pytorch의 `dataloader`를 이용하기 위해서는 `torch.utils.data.Dataset` 클래스를 상속한 클래스의 선언이 필요하다.\n",
        "- 클래스 내에 `__init__`, `__getitem__`, `__len__`의 3개의 메소드를 선언하여 오버라이드한다."
      ],
      "metadata": {
        "id": "i837jqmdjrGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class semiconductorDataset(Dataset): # inheritin from Dataset class\n",
        "\n",
        "    def __init__(self, csv_file, root_dir=\"\", transform=None):\n",
        "        self.annotation_df = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir # root directory of images, leave \"\" if using the image path column in the __getitem__ method\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotation_df) # return length (numer of rows) of the dataframe\n",
        "\n",
        "    ############ 실습 : annotation_df에 있는 이미지를 읽어 들어서, 변경한후\n",
        "    ############        읽어 들인 값을 return 하는 함수를 작성한다. \n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        ##### Image 읽기 \n",
        "        image_path = os.path.join(self.root_dir, self.annotation_df.iloc[idx, 1]) #use image path column (index = 1) in csv file\n",
        "        image = cv2.imread(image_path) # read image by cv2\n",
        "        #### 이미지를 Channel순서를 변경\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # convert from BGR to RGB for matplotlib\n",
        "        #### 이미지 Transform \n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        #### 이미지에 대한 추가 정보 확인 \n",
        "        class_name = self.annotation_df.iloc[idx, 2] # use class name column (index = 2) in csv file\n",
        "        class_index = self.annotation_df.iloc[idx, 3] # use class index column (index = 3) in csv file\n",
        "\n",
        "        return image, class_name, class_index"
      ],
      "metadata": {
        "id": "qOUmcMyjYn-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(), # PIL Image를 Tensor로 변경 \n",
        "    transforms.Resize((100,100)), # 크기 변경\n",
        "    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)), # Normalize\n",
        "    transforms.Grayscale(), # Gray Scale로 변경 \n",
        "    transforms.RandomAdjustSharpness(sharpness_factor=2), # randomly adjusts the sharpness\n",
        "    transforms.RandomAutocontrast() # 임의로 Contrast변경\n",
        "])\n",
        "\n",
        "##### 실습 : train_dataset 선언\n",
        "train_dataset = semiconductorDataset(csv_file='train.csv', root_dir=\"\", transform=transform)"
      ],
      "metadata": {
        "id": "aH1Zej7hLxxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batch 단위 학습을 위한 DataLoader 선언\n",
        " - Dataset내부의 샘플들을 batch 크기로 추출\n",
        " - Batch Size는 1step에 들어간 데이터의 개수\n",
        " - Epoch 마다 데이터를 섞어(Shuffle) Overfitting을 방지 \n",
        " - 병렬처리를 지원하여 데이터 검색 속도를 향상\n"
      ],
      "metadata": {
        "id": "m5pV5UK4kBiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### 실습 : torch.utils.DataLoader를 이용하여 데이터를 load. Batch 크기를 10으로 한다. \n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=10, shuffle=True, num_workers=2)\n",
        "\n",
        "for i, data in enumerate(train_dataloader):\n",
        "  images, class_name, labels = data\n",
        "  print(images.shape, labels.shape)\n",
        "\n",
        "  # 5번만 데이터를 load하고 멈춘다\n",
        "  if i > 3:\n",
        "    break"
      ],
      "metadata": {
        "id": "bCTnlT1pRpSS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "231a3249-e9ea-4e6e-9bb6-b0760f5c6cd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 1, 100, 100]) torch.Size([10])\n",
            "torch.Size([10, 1, 100, 100]) torch.Size([10])\n",
            "torch.Size([10, 1, 100, 100]) torch.Size([10])\n",
            "torch.Size([10, 1, 100, 100]) torch.Size([10])\n",
            "torch.Size([10, 1, 100, 100]) torch.Size([10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VGG6를 이용하여 학습을 수행한다\n",
        "![](https://drive.google.com/uc?export=view&id=1vLrvhxczx1ZCOH05cxzzESylaOaF1Uj2)\n"
      ],
      "metadata": {
        "id": "WhDfo_bXniHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class VGG_BatchNormTorch(nn.Module):\n",
        "  def __init__(self, in_channels, out_features):\n",
        "    super(VGG_BatchNormTorch, self).__init__()\n",
        "\n",
        "    ###### 실습 : Gray이미지 이므로 in_channel =1 로 선언한다\n",
        "    self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=3, padding=1) \n",
        "    self.norm1 = torch.nn.BatchNorm2d(32)\n",
        "    self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n",
        "    self.norm2 = torch.nn.BatchNorm2d(32)\n",
        "    \n",
        "    self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "    self.norm3 = torch.nn.BatchNorm2d(64)\n",
        "    \n",
        "    self.conv4 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
        "    self.norm4 = torch.nn.BatchNorm2d(64)\n",
        "\n",
        "    self.conv5 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
        "    #### 실습 : Batch Norm 을 Pytorch의 값으로 변경한다 \n",
        "    self.norm5 = torch.nn.BatchNorm2d(128)\n",
        "    self.conv6 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n",
        "    #### 실습 : Batch Norm 을 Pytorch의 값으로 변경한다 \n",
        "    self.norm6 = torch.nn.BatchNorm2d(128)\n",
        "\n",
        "    self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2) #Maxpooling layer to change feature size \n",
        "    self.avg_pool = nn.AdaptiveAvgPool2d(output_size = (1, 1)) #Note that average pooling layer is not adopted in original VGG architecture. We use average pooling layer to make the architecture for experiment simple.\n",
        "\n",
        "    ###### 실습 : 11개의 Label이 있으므로 out_features=11로 선언한다\n",
        "    self.fc = nn.Linear(in_features=128, out_features=out_features)\n",
        "\n",
        "  def forward(self, x):\n",
        "    #### 실습 : Batch Norm을 Convolution 이후에 선언 \n",
        "    x = self.norm1(self.conv1(x))\n",
        "    x = F.relu(x)\n",
        "    x = self.norm2(self.conv2(x))\n",
        "    x = F.relu(x)\n",
        "    x = self.max_pool(x)\n",
        "\n",
        "    x = self.norm3(self.conv3(x))\n",
        "    x = F.relu(x)\n",
        "    x = self.norm4(self.conv4(x))\n",
        "    x = F.relu(x)\n",
        "    x = self.max_pool(x)\n",
        "\n",
        "    x = self.norm5(self.conv5(x))\n",
        "    x = F.relu(x)\n",
        "    x = self.norm6(self.conv6(x))\n",
        "    x = F.relu(x)\n",
        "\n",
        "    x = self.avg_pool(x)\n",
        "    x = x.view(-1, 128)\n",
        "    x = self.fc(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "QY8Iolz6gvHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, data_loader, criterion, optimizer, n_epoch):\n",
        "  \n",
        "  model.train() #\n",
        "  for epoch in range(n_epoch):\n",
        "    running_loss = 0\n",
        "    ### 실습 : Custom dataloader 에서 값을 가져온다\n",
        "    for i, (images, _, labels) in enumerate(data_loader):\n",
        "      images, labels = images.cuda(), labels.cuda()\n",
        "      outputs = model(images)\n",
        "      loss = criterion(outputs, labels)\n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      running_loss += loss.item()\n",
        "      if (i + 1) % 100 == 0:\n",
        "        print('iteration: [{}/{}]'.format(i + 1, len(data_loader)))\n",
        "          \n",
        "    print('Epoch {}, loss = {:.3f}'.format(epoch + 1, running_loss/len(data_loader)))"
      ],
      "metadata": {
        "id": "yHya_ZR3vj0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval(model, data_loader):\n",
        "  \n",
        "  model.eval()\n",
        "  total = 0\n",
        "  correct = 0\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    ### 실습 : Custom dataloader 에서 값을 가져온다\n",
        "    for images, _,  labels in data_loader:\n",
        "      images, labels = images.cuda(), labels.cuda()\n",
        "      outputs = model(images)\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "      \n",
        "  print('Test Accuracy: {}%'.format(accuracy))"
      ],
      "metadata": {
        "id": "HjX476cmvhSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reset_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)"
      ],
      "metadata": {
        "id": "RLI00mauwDyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reset_seed(2020)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "vgg_batchnorm_model = VGG_BatchNormTorch(in_channels=1, out_features=11).to(\"cuda\")\n",
        "optimizer = optim.Adam(params=vgg_batchnorm_model.parameters())\n",
        "\n",
        "train(vgg_batchnorm_model, train_dataloader, criterion, optimizer, n_epoch=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t85o-4CZve4T",
        "outputId": "eb54cd82-e5e6-4826-cd63-ae291fb9bb9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, loss = 2.432\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 정확도를 평가해 봅시다 \n",
        "* 데이터가 너무 적어서 평가 데이터에 대한 정확도를 봅니다"
      ],
      "metadata": {
        "id": "8d4AUYGHJzQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval(vgg_batchnorm_model, train_dataloader)"
      ],
      "metadata": {
        "id": "cdj0G1XrrBkv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18eebbd7-9f2c-4842-9bf3-eefdfcccdd09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 93.93939393939394%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RGB로 학습 후 평가해 본다"
      ],
      "metadata": {
        "id": "qJ-2OryDzP4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(), # PIL Image를 Tensor로 변경 \n",
        "    transforms.Resize((100,100)), # 크기 변경\n",
        "    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)), # Normalize\n",
        "    # transforms.Grayscale(), # Gray Scale로 변경 \n",
        "    transforms.RandomAdjustSharpness(sharpness_factor=2), # randomly adjusts the sharpness\n",
        "    transforms.RandomAutocontrast() # 임의로 Contrast변경\n",
        "])\n",
        "\n",
        "train_dataset = semiconductorDataset(csv_file='train.csv', root_dir=\"\", transform=transform)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=10, shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "id": "k-skeZ8GzPQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reset_seed(2020)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "############### 실습 : 입력 Channel의 개수는 3\n",
        "vgg_batchnorm_model = VGG_BatchNormTorch(in_channels=3, out_features=11).to(\"cuda\")\n",
        "optimizer = optim.Adam(params=vgg_batchnorm_model.parameters())\n",
        "\n",
        "train(vgg_batchnorm_model, train_dataloader, criterion, optimizer, n_epoch=40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4PSMYpRzQ0B",
        "outputId": "44deb714-3a05-48b2-cccd-20190dc0a0c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, loss = 2.386\n",
            "Epoch 2, loss = 2.198\n",
            "Epoch 3, loss = 2.091\n",
            "Epoch 4, loss = 2.011\n",
            "Epoch 5, loss = 2.010\n",
            "Epoch 6, loss = 2.032\n",
            "Epoch 7, loss = 1.978\n",
            "Epoch 8, loss = 1.921\n",
            "Epoch 9, loss = 1.898\n",
            "Epoch 10, loss = 1.828\n",
            "Epoch 11, loss = 1.734\n",
            "Epoch 12, loss = 1.599\n",
            "Epoch 13, loss = 1.538\n",
            "Epoch 14, loss = 1.479\n",
            "Epoch 15, loss = 1.471\n",
            "Epoch 16, loss = 1.467\n",
            "Epoch 17, loss = 1.420\n",
            "Epoch 18, loss = 1.364\n",
            "Epoch 19, loss = 1.196\n",
            "Epoch 20, loss = 1.221\n",
            "Epoch 21, loss = 1.205\n",
            "Epoch 22, loss = 1.178\n",
            "Epoch 23, loss = 1.163\n",
            "Epoch 24, loss = 1.108\n",
            "Epoch 25, loss = 1.156\n",
            "Epoch 26, loss = 1.029\n",
            "Epoch 27, loss = 0.925\n",
            "Epoch 28, loss = 0.840\n",
            "Epoch 29, loss = 0.848\n",
            "Epoch 30, loss = 0.819\n",
            "Epoch 31, loss = 0.772\n",
            "Epoch 32, loss = 0.832\n",
            "Epoch 33, loss = 0.745\n",
            "Epoch 34, loss = 0.805\n",
            "Epoch 35, loss = 0.824\n",
            "Epoch 36, loss = 0.665\n",
            "Epoch 37, loss = 0.631\n",
            "Epoch 38, loss = 0.593\n",
            "Epoch 39, loss = 0.625\n",
            "Epoch 40, loss = 0.592\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval(vgg_batchnorm_model, train_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HtIt4Va0KmL",
        "outputId": "5253edef-c172-4e41-c8a6-02ab22aaeaf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 65.15151515151516%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########### 실습 : 분류 성능을 평가해 봅니다. \n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(test_labels, prediction_label))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvVspfx1f9tk",
        "outputId": "0dd95ccb-3d3e-4f81-dd00-515e04a251e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-cf9b68e43883>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m########### 실습 : 분류 성능을 평가해 봅니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'test_labels' is not defined"
          ]
        }
      ]
    }
  ]
}