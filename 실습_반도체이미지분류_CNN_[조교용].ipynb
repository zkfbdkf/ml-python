{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO2jc8cpFFNzoiJ4EwARIa0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chminPark/ml-python/blob/master/%EC%8B%A4%EC%8A%B5_%EB%B0%98%EB%8F%84%EC%B2%B4%EC%9D%B4%EB%AF%B8%EC%A7%80%EB%B6%84%EB%A5%98_CNN_%5B%EC%A1%B0%EA%B5%90%EC%9A%A9%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1nOUWByW6ifnd_P-Jgd4mbNkAT-jGzpWC\n",
        "!mkdir 'semicon'\n",
        "!unzip -q semiconductor_dataset.zip -d semicon\n",
        "!rm semiconductor_dataset.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvM0McnQ8Mx8",
        "outputId": "428d4491-fcf6-4be4-864b-978ea846ad58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1nOUWByW6ifnd_P-Jgd4mbNkAT-jGzpWC\n",
            "To: /content/semiconductor_dataset.zip\n",
            "100% 10.3M/10.3M [00:00<00:00, 214MB/s]\n",
            "mkdir: cannot create directory ‘semicon’: File exists\n",
            "replace semicon/abnorm_1/1-4.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4a6_fD3mlZOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import os\n",
        "import pandas as pd\n",
        "\"\"\"\n",
        "Args:\n",
        "  directory_string: 이미지가 저장되어 있는 폴더 Path\n",
        "  output_csv_name: csv 파일 이름\n",
        "Returns:\n",
        "  csv file\n",
        "\"\"\"\n",
        "def build_csv(directory_string, output_csv_name):\n",
        "\n",
        "    directory = directory_string\n",
        "    class_list = os.listdir(directory)\n",
        "    class_list.sort()\n",
        "\n",
        "\n",
        "    with open(output_csv_name, 'w', newline='') as csvfile:\n",
        "\n",
        "        ### 실습 : CSV 파일 Object 생성\n",
        "        writer = csv.writer(csvfile, delimiter=',')\n",
        "        ############################################\n",
        "        writer.writerow(['file_name', 'file_path', 'class_name', 'class_index']) # CSV의 column 이름을 지정\n",
        "\n",
        "        ###### 각 folder에 들어가서 각 이미지의 이름을 가져옴\n",
        "        for class_name in class_list:\n",
        "          class_path = os.path.join(directory, class_name)\n",
        "          file_list = os.listdir(class_path) # 해당 파일 내부의 이미지를 확보\n",
        "          for file_name in file_list:\n",
        "              file_path = os.path.join(directory, class_name, file_name) #concatenate class folder dir, class name and file name\n",
        "              writer.writerow([file_name, file_path, class_name, class_name.split(\"_\")[1]]) #write the file path and class name to the csv file\n",
        "        #############################\n",
        "\n",
        "    return\n",
        "\n",
        "train_folder = os.path.join(os.getcwd(), 'semicon')\n",
        "build_csv(train_folder, 'train.csv')\n",
        "train_df = pd.read_csv('train.csv')\n"
      ],
      "metadata": {
        "id": "5dXEooJlWsRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom데이터를 활용하기 위한 Dataset을 선언\n",
        "- pytorch의 `dataloader`를 이용하기 위해서는 `torch.utils.data.Dataset` 클래스를 상속한 클래스의 선언이 필요하다.\n",
        "- 클래스 내에 `__init__`, `__getitem__`, `__len__`의 3개의 메소드를 선언하여 오버라이드한다."
      ],
      "metadata": {
        "id": "i837jqmdjrGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class semiconductorDataset(Dataset): # inheritin from Dataset class\n",
        "\n",
        "    def __init__(self, csv_file, root_dir=\"\", transform=None):\n",
        "        self.annotation_df = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir # root directory of images, leave \"\" if using the image path column in the __getitem__ method\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotation_df) # return length (numer of rows) of the dataframe\n",
        "\n",
        "    ############ 실습 : annotation_df에 있는 이미지를 읽어 들어서, 변경한후\n",
        "    ############        읽어 들인 값을 return 하는 함수를 작성한다.\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        ##### Image 읽기\n",
        "        image_path = os.path.join(self.root_dir, self.annotation_df.iloc[idx, 1]) #use image path column (index = 1) in csv file\n",
        "        image = cv2.imread(image_path) # read image by cv2\n",
        "        #### 이미지를 Channel순서를 변경\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # convert from BGR to RGB for matplotlib\n",
        "        #### 이미지 Transform\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        #### 이미지에 대한 추가 정보 확인\n",
        "        class_name = self.annotation_df.iloc[idx, 2] # use class name column (index = 2) in csv file\n",
        "        class_index = self.annotation_df.iloc[idx, 3] # use class index column (index = 3) in csv file\n",
        "\n",
        "        return image, class_name, class_index"
      ],
      "metadata": {
        "id": "qOUmcMyjYn-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(), # PIL Image를 Tensor로 변경\n",
        "    transforms.Resize((100,100), antialias=True ), # 크기 변경\n",
        "    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)), # Normalize\n",
        "    transforms.Grayscale(), # Gray Scale로 변경\n",
        "    transforms.RandomAdjustSharpness(sharpness_factor=2), # randomly adjusts the sharpness\n",
        "    transforms.RandomAutocontrast() # 임의로 Contrast변경\n",
        "])\n",
        "\n",
        "##### 실습 : train_dataset 선언\n",
        "train_dataset = semiconductorDataset(csv_file='train.csv', root_dir=\"\", transform=transform)"
      ],
      "metadata": {
        "id": "aH1Zej7hLxxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batch 단위 학습을 위한 DataLoader 선언\n",
        " - Dataset내부의 샘플들을 batch 크기로 추출\n",
        " - Batch Size는 1step에 들어간 데이터의 개수\n",
        " - Epoch 마다 데이터를 섞어(Shuffle) Overfitting을 방지\n",
        " - 병렬처리를 지원하여 데이터 검색 속도를 향상\n"
      ],
      "metadata": {
        "id": "m5pV5UK4kBiR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True)."
      ],
      "metadata": {
        "id": "IDfNgkCUoB00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### 실습 : torch.utils.DataLoader를 이용하여 데이터를 load. Batch 크기를 10으로 한다.\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=10, shuffle=True, num_workers=2)\n",
        "\n",
        "for i, data in enumerate(train_dataloader):\n",
        "  images, class_name, labels = data\n",
        "  print(images.shape, labels.shape)\n",
        "\n",
        "  # 5번만 데이터를 load하고 멈춘다\n",
        "  if i > 3:\n",
        "    break"
      ],
      "metadata": {
        "id": "bCTnlT1pRpSS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "367e6aed-29bc-4fb6-f5d7-2894efef843f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 1, 100, 100]) torch.Size([10])\n",
            "torch.Size([10, 1, 100, 100]) torch.Size([10])\n",
            "torch.Size([10, 1, 100, 100]) torch.Size([10])\n",
            "torch.Size([10, 1, 100, 100]) torch.Size([10])\n",
            "torch.Size([10, 1, 100, 100]) torch.Size([10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VGG6를 이용하여 학습을 수행한다\n",
        "![](https://drive.google.com/uc?export=view&id=1vLrvhxczx1ZCOH05cxzzESylaOaF1Uj2)\n"
      ],
      "metadata": {
        "id": "WhDfo_bXniHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class VGG_BatchNormTorch(nn.Module):\n",
        "  def __init__(self, in_channels, out_features):\n",
        "    super(VGG_BatchNormTorch, self).__init__()\n",
        "\n",
        "    ###### 실습 : Gray이미지 이므로 in_channel =1 로 선언한다\n",
        "    self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=3, padding=1)\n",
        "    self.norm1 = torch.nn.BatchNorm2d(32)\n",
        "    self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n",
        "    self.norm2 = torch.nn.BatchNorm2d(32)\n",
        "\n",
        "    self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "    self.norm3 = torch.nn.BatchNorm2d(64)\n",
        "\n",
        "    self.conv4 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
        "    self.norm4 = torch.nn.BatchNorm2d(64)\n",
        "\n",
        "    self.conv5 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
        "    #### 실습 : Batch Norm 을 Pytorch의 값으로 변경한다\n",
        "    self.norm5 = torch.nn.BatchNorm2d(128)\n",
        "    self.conv6 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n",
        "    #### 실습 : Batch Norm 을 Pytorch의 값으로 변경한다\n",
        "    self.norm6 = torch.nn.BatchNorm2d(128)\n",
        "\n",
        "    self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2) #Maxpooling layer to change feature size\n",
        "    self.avg_pool = nn.AdaptiveAvgPool2d(output_size = (1, 1)) #Note that average pooling layer is not adopted in original VGG architecture. We use average pooling layer to make the architecture for experiment simple.\n",
        "\n",
        "    ###### 실습 : 11개의 Label이 있으므로 out_features=11로 선언한다\n",
        "    self.fc = nn.Linear(in_features=128, out_features=out_features)\n",
        "\n",
        "  def forward(self, x):\n",
        "    #### 실습 : Batch Norm을 Convolution 이후에 선언\n",
        "    x = self.norm1(self.conv1(x))\n",
        "    x = F.relu(x)\n",
        "    x = self.norm2(self.conv2(x))\n",
        "    x = F.relu(x)\n",
        "    x = self.max_pool(x)\n",
        "\n",
        "    x = self.norm3(self.conv3(x))\n",
        "    x = F.relu(x)\n",
        "    x = self.norm4(self.conv4(x))\n",
        "    x = F.relu(x)\n",
        "    x = self.max_pool(x)\n",
        "\n",
        "    x = self.norm5(self.conv5(x))\n",
        "    x = F.relu(x)\n",
        "    x = self.norm6(self.conv6(x))\n",
        "    x = F.relu(x)\n",
        "\n",
        "    x = self.avg_pool(x)\n",
        "    x = x.view(-1, 128)\n",
        "    x = self.fc(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "QY8Iolz6gvHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, data_loader, criterion, optimizer, n_epoch):\n",
        "\n",
        "  model.train() #\n",
        "  for epoch in range(n_epoch):\n",
        "    running_loss = 0\n",
        "    ### 실습 : Custom dataloader 에서 값을 가져온다\n",
        "    for i, (images, _, labels) in enumerate(data_loader):\n",
        "      images, labels = images.cuda(), labels.cuda()\n",
        "      outputs = model(images)\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      running_loss += loss.item()\n",
        "      if (i + 1) % 100 == 0:\n",
        "        print('iteration: [{}/{}]'.format(i + 1, len(data_loader)))\n",
        "\n",
        "    print('Epoch {}, loss = {:.3f}'.format(epoch + 1, running_loss/len(data_loader)))"
      ],
      "metadata": {
        "id": "yHya_ZR3vj0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval(model, data_loader):\n",
        "\n",
        "  model.eval()\n",
        "  total = 0\n",
        "  correct = 0\n",
        "\n",
        "  preds = []\n",
        "  trues = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    ### 실습 : Custom dataloader 에서 값을 가져온다\n",
        "    for images, _,  labels in data_loader:\n",
        "      images, labels = images.cuda(), labels.cuda()\n",
        "      outputs = model(images)\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "      preds.append(predicted.detach().cpu().numpy())\n",
        "      trues.append(labels.detach().cpu().numpy())\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "  print('Test Accuracy: {}%'.format(accuracy))\n",
        "  return preds, trues\n"
      ],
      "metadata": {
        "id": "HjX476cmvhSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reset_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)"
      ],
      "metadata": {
        "id": "RLI00mauwDyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reset_seed(2020)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "vgg_batchnorm_model = VGG_BatchNormTorch(in_channels=1, out_features=11).to(\"cuda\")\n",
        "optimizer = optim.Adam(params=vgg_batchnorm_model.parameters())\n",
        "\n",
        "train(vgg_batchnorm_model, train_dataloader, criterion, optimizer, n_epoch=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t85o-4CZve4T",
        "outputId": "8083dcdb-25fd-496b-8317-8c4225f634a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, loss = 2.419\n",
            "Epoch 2, loss = 2.259\n",
            "Epoch 3, loss = 2.087\n",
            "Epoch 4, loss = 2.034\n",
            "Epoch 5, loss = 1.968\n",
            "Epoch 6, loss = 1.843\n",
            "Epoch 7, loss = 1.745\n",
            "Epoch 8, loss = 1.699\n",
            "Epoch 9, loss = 1.588\n",
            "Epoch 10, loss = 1.443\n",
            "Epoch 11, loss = 1.361\n",
            "Epoch 12, loss = 1.246\n",
            "Epoch 13, loss = 1.287\n",
            "Epoch 14, loss = 1.217\n",
            "Epoch 15, loss = 1.179\n",
            "Epoch 16, loss = 1.167\n",
            "Epoch 17, loss = 1.094\n",
            "Epoch 18, loss = 1.014\n",
            "Epoch 19, loss = 0.995\n",
            "Epoch 20, loss = 0.961\n",
            "Epoch 21, loss = 0.843\n",
            "Epoch 22, loss = 0.794\n",
            "Epoch 23, loss = 0.784\n",
            "Epoch 24, loss = 0.674\n",
            "Epoch 25, loss = 0.640\n",
            "Epoch 26, loss = 0.603\n",
            "Epoch 27, loss = 0.596\n",
            "Epoch 28, loss = 0.491\n",
            "Epoch 29, loss = 0.457\n",
            "Epoch 30, loss = 0.420\n",
            "Epoch 31, loss = 0.399\n",
            "Epoch 32, loss = 0.333\n",
            "Epoch 33, loss = 0.351\n",
            "Epoch 34, loss = 0.339\n",
            "Epoch 35, loss = 0.291\n",
            "Epoch 36, loss = 0.274\n",
            "Epoch 37, loss = 0.222\n",
            "Epoch 38, loss = 0.240\n",
            "Epoch 39, loss = 0.216\n",
            "Epoch 40, loss = 0.236\n",
            "Epoch 41, loss = 0.174\n",
            "Epoch 42, loss = 0.216\n",
            "Epoch 43, loss = 0.199\n",
            "Epoch 44, loss = 0.170\n",
            "Epoch 45, loss = 0.137\n",
            "Epoch 46, loss = 0.102\n",
            "Epoch 47, loss = 0.091\n",
            "Epoch 48, loss = 0.097\n",
            "Epoch 49, loss = 0.090\n",
            "Epoch 50, loss = 0.120\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 정확도를 평가해 봅시다\n",
        "* 데이터가 너무 적어서 학습 데이터에 대한 정확도를 봅니다"
      ],
      "metadata": {
        "id": "8d4AUYGHJzQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds, trues = eval(vgg_batchnorm_model, train_dataloader)"
      ],
      "metadata": {
        "id": "cdj0G1XrrBkv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21a9a0b6-c497-4437-ae64-501dc0e0a974"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 59.5959595959596%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_label = np.stack(preds[0:-1]).flatten()\n",
        "train_labels = np.stack(trues[0:-1]).flatten()\n",
        "prediction_label, train_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSM2yGiFqD7t",
        "outputId": "d48deb85-f0d9-4ce5-fbf7-a94eb65d5856"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 8,  3,  3,  2,  0,  8,  7,  8,  4,  5,  8,  5, 10,  5,  5,  0,  3,\n",
              "         0,  9,  3,  8,  4,  5,  7,  5,  2,  7,  2, 10,  2,  9,  2,  2,  8,\n",
              "         7, 10,  2,  5,  5,  5,  8,  8,  2,  7,  7,  2,  8, 10,  8,  8,  5,\n",
              "         4,  5, 10,  0,  7,  5,  0, 10,  7,  5,  7,  5,  3,  5,  4,  9,  7,\n",
              "         9,  7,  2,  5, 10,  5,  2,  2,  5,  7,  4,  5,  2,  7,  5,  5,  3,\n",
              "         9,  2,  7,  8,  9,  8,  4,  9,  8,  3,  1,  5,  9,  5,  1, 10,  5,\n",
              "         8,  2,  0,  9,  7,  9,  8,  8,  0, 10,  5,  2,  4,  8,  7,  8,  0,\n",
              "         4,  7,  5,  5,  7,  3,  2, 10,  3,  1,  3,  5, 10,  4,  9,  4,  3,\n",
              "         2,  3, 10,  2, 10,  3,  3,  5,  2,  0, 10,  9,  7,  7,  8,  9,  5,\n",
              "         9,  4,  2, 10,  7,  2,  5,  7,  7,  9,  5,  8,  0,  4,  9,  2,  8,\n",
              "         4,  5,  7,  6,  2,  4,  7,  9,  8,  4,  3,  9,  2,  7,  2,  5,  9,\n",
              "         7,  7, 10]),\n",
              " array([ 1,  6,  6,  1,  0,  8,  7,  8,  4,  5,  8,  5, 10,  5,  5,  2,  3,\n",
              "         0,  9,  3,  7,  4,  0,  7,  5,  2,  7,  2, 10,  1,  0,  2,  2,  0,\n",
              "         7, 10,  1,  5,  9,  6,  0,  8,  2,  2,  9,  2,  7, 10,  8,  1,  5,\n",
              "         4,  5, 10,  9,  7,  6,  0, 10,  7,  5,  8,  5,  3,  3,  4,  9,  8,\n",
              "         9,  7,  2,  6, 10,  6,  1,  1,  6,  8,  4,  5,  2,  7,  5,  5,  3,\n",
              "         9,  1,  8,  4,  8,  8,  4,  0,  8,  6,  1,  5,  9,  3,  1, 10,  6,\n",
              "         2,  1,  5,  8,  7,  0,  0,  9,  3, 10,  3,  2,  4,  7,  7,  8,  0,\n",
              "         4,  8,  3,  6,  7,  6,  2, 10,  3,  1,  3,  5, 10,  4,  9,  4,  3,\n",
              "         1,  3, 10,  2, 10,  3,  3,  4,  2,  5, 10,  0,  7,  2,  4,  0,  6,\n",
              "         9,  4,  1, 10,  0,  2,  8,  0,  8,  9,  6,  9,  0,  6,  9,  1,  0,\n",
              "         4,  6,  2,  6,  2,  4,  8,  0,  9,  4,  3,  9,  1,  7,  1,  6,  9,\n",
              "         7,  9, 10]))"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########### 실습 : 분류 성능을 평가해 봅니다.\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(train_labels, prediction_label))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSkrsaxvo2Sg",
        "outputId": "0d6afd22-8808-4e04-d8de-bc953461ccdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.28      0.36        18\n",
            "           1       1.00      0.18      0.30        17\n",
            "           2       0.54      0.74      0.62        19\n",
            "           3       0.73      0.69      0.71        16\n",
            "           4       0.93      0.82      0.87        17\n",
            "           5       0.44      0.88      0.59        17\n",
            "           6       1.00      0.06      0.11        17\n",
            "           7       0.50      0.82      0.62        17\n",
            "           8       0.35      0.44      0.39        18\n",
            "           9       0.58      0.61      0.59        18\n",
            "          10       1.00      1.00      1.00        16\n",
            "\n",
            "    accuracy                           0.59       190\n",
            "   macro avg       0.69      0.59      0.56       190\n",
            "weighted avg       0.68      0.59      0.56       190\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RGB로 학습 후 평가해 본다"
      ],
      "metadata": {
        "id": "qJ-2OryDzP4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(), # PIL Image를 Tensor로 변경\n",
        "    transforms.Resize((100,100), antialias=True), # 크기 변경\n",
        "    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)), # Normalize\n",
        "    # transforms.Grayscale(), # Gray Scale로 변경\n",
        "    transforms.RandomAdjustSharpness(sharpness_factor=2), # randomly adjusts the sharpness\n",
        "    transforms.RandomAutocontrast() # 임의로 Contrast변경\n",
        "])\n",
        "\n",
        "train_dataset = semiconductorDataset(csv_file='train.csv', root_dir=\"\", transform=transform)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=10, shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "id": "k-skeZ8GzPQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reset_seed(2020)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "############### 실습 : 입력 Channel의 개수는 3\n",
        "vgg_batchnorm_model = VGG_BatchNormTorch(in_channels=3, out_features=11).to(\"cuda\")\n",
        "optimizer = optim.Adam(params=vgg_batchnorm_model.parameters())\n",
        "\n",
        "train(vgg_batchnorm_model, train_dataloader, criterion, optimizer, n_epoch=40)"
      ],
      "metadata": {
        "id": "E4PSMYpRzQ0B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "166fe687-9430-4312-8773-76123750fb13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, loss = 2.355\n",
            "Epoch 2, loss = 2.175\n",
            "Epoch 3, loss = 2.043\n",
            "Epoch 4, loss = 2.013\n",
            "Epoch 5, loss = 1.979\n",
            "Epoch 6, loss = 1.940\n",
            "Epoch 7, loss = 1.963\n",
            "Epoch 8, loss = 1.943\n",
            "Epoch 9, loss = 1.831\n",
            "Epoch 10, loss = 1.744\n",
            "Epoch 11, loss = 1.613\n",
            "Epoch 12, loss = 1.581\n",
            "Epoch 13, loss = 1.603\n",
            "Epoch 14, loss = 1.505\n",
            "Epoch 15, loss = 1.454\n",
            "Epoch 16, loss = 1.518\n",
            "Epoch 17, loss = 1.431\n",
            "Epoch 18, loss = 1.359\n",
            "Epoch 19, loss = 1.335\n",
            "Epoch 20, loss = 1.270\n",
            "Epoch 21, loss = 1.223\n",
            "Epoch 22, loss = 1.171\n",
            "Epoch 23, loss = 1.240\n",
            "Epoch 24, loss = 1.165\n",
            "Epoch 25, loss = 1.178\n",
            "Epoch 26, loss = 1.125\n",
            "Epoch 27, loss = 1.068\n",
            "Epoch 28, loss = 1.081\n",
            "Epoch 29, loss = 1.050\n",
            "Epoch 30, loss = 1.068\n",
            "Epoch 31, loss = 0.994\n",
            "Epoch 32, loss = 0.901\n",
            "Epoch 33, loss = 0.861\n",
            "Epoch 34, loss = 0.960\n",
            "Epoch 35, loss = 0.912\n",
            "Epoch 36, loss = 0.848\n",
            "Epoch 37, loss = 0.928\n",
            "Epoch 38, loss = 0.858\n",
            "Epoch 39, loss = 0.888\n",
            "Epoch 40, loss = 0.805\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds, trues = eval(vgg_batchnorm_model, train_dataloader)"
      ],
      "metadata": {
        "id": "5HtIt4Va0KmL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf073407-bd05-4915-a741-ccfeb7daa6ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 69.6969696969697%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_label = np.stack(preds[0:-1]).flatten()\n",
        "train_labels = np.stack(trues[0:-1]).flatten()\n",
        "prediction_label, train_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBqwZIK7qtZs",
        "outputId": "c1547909-09cd-46cd-d622-68f9fd4ab76b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 4,  9,  3,  7,  2,  9,  5, 10,  4,  2,  6,  2,  2,  4,  6,  9,  6,\n",
              "         6,  1,  9,  6,  1,  2,  0, 10,  5,  9,  9,  4,  1,  2,  4,  6,  2,\n",
              "         6,  2, 10,  3,  2,  2,  7,  3,  9,  5,  9,  6,  5,  9,  6,  3,  2,\n",
              "         9,  6,  6,  4, 10,  3,  9,  9,  3,  3,  2,  6,  9,  2,  5,  9,  2,\n",
              "         4,  5,  3, 10,  1,  6,  2,  0,  0,  0,  2, 10,  2,  2,  4,  5,  3,\n",
              "         2,  2,  3,  3,  0,  5,  2,  6, 10, 10,  2,  6,  9,  3,  3,  0,  2,\n",
              "         3,  9,  9,  2,  9,  9,  3,  2,  0,  4,  9,  2,  1,  1,  6,  4,  9,\n",
              "         7,  0,  9,  2,  2,  2,  9, 10,  5,  9,  0, 10,  5,  9,  2,  2,  9,\n",
              "         1,  9,  3,  3, 10,  0, 10,  7,  2,  2, 10,  9,  2,  9,  5,  2,  2,\n",
              "         2,  9,  2,  4,  9,  0,  2,  4,  2,  5,  0,  6,  9,  5,  4, 10,  9,\n",
              "         9, 10,  9,  0,  9,  9,  4,  5,  4, 10,  4,  2,  0,  2,  9,  7,  1,\n",
              "         5,  2, 10]),\n",
              " array([ 4,  9,  3,  8,  2,  8,  5, 10,  4,  1,  6,  7,  1,  4,  6,  7,  6,\n",
              "         6,  1,  0,  6,  1,  2,  2, 10,  5,  7,  7,  4,  1,  1,  4,  6,  2,\n",
              "         6,  7, 10,  3,  2,  1,  0,  3,  7,  5,  9,  6,  5,  8,  6,  3,  2,\n",
              "         7,  6,  6,  4, 10,  3,  8,  9,  3,  3,  2,  6,  9,  0,  5,  0,  1,\n",
              "         4,  5,  3, 10,  1,  6,  2,  0,  0,  8,  2, 10,  8,  8,  4,  5,  3,\n",
              "         0,  0,  3,  3,  8,  5,  8,  6, 10, 10,  8,  6,  0,  3,  3,  8,  2,\n",
              "         3,  9,  9,  8,  9,  9,  3,  2,  2,  4,  9,  1,  1,  1,  6,  4,  9,\n",
              "         7,  7,  0,  2,  2,  0,  9, 10,  5,  9,  2, 10,  5,  7,  8,  1,  1,\n",
              "         1,  0,  3,  3, 10,  8, 10,  7,  0,  8, 10,  9,  2,  9,  5,  7,  2,\n",
              "         0,  7,  7,  4,  9,  0,  1,  4,  2,  5,  0,  6,  7,  5,  4, 10,  9,\n",
              "         7, 10,  0,  8,  9,  9,  4,  5,  4, 10,  4,  0,  0,  2,  8,  7,  1,\n",
              "         5,  8, 10]))"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    }
  ]
}