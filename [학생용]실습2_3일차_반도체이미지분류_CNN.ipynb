{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPbhzXfIp+d0CSjIJW5gso4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chminPark/ml-python/blob/master/%5B%ED%95%99%EC%83%9D%EC%9A%A9%5D%EC%8B%A4%EC%8A%B52_3%EC%9D%BC%EC%B0%A8_%EB%B0%98%EB%8F%84%EC%B2%B4%EC%9D%B4%EB%AF%B8%EC%A7%80%EB%B6%84%EB%A5%98_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1nOUWByW6ifnd_P-Jgd4mbNkAT-jGzpWC\n",
        "!mkdir 'semicon'\n",
        "!unzip -q semiconductor_dataset.zip -d semicon\n",
        "!rm semiconductor_dataset.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvM0McnQ8Mx8",
        "outputId": "e83e2655-3f8a-4107-bf8a-b240192f1069"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gdown/cli.py:127: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1nOUWByW6ifnd_P-Jgd4mbNkAT-jGzpWC\n",
            "To: /content/semiconductor_dataset.zip\n",
            "100% 10.3M/10.3M [00:00<00:00, 78.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import os\n",
        "import pandas as pd\n",
        "\"\"\" \n",
        "Args:    \n",
        "  directory_string: 이미지가 저장되어 있는 폴더 Path\n",
        "  output_csv_name: csv 파일 이름\n",
        "Returns:\n",
        "  csv file \n",
        "\"\"\"\n",
        "def build_csv(directory_string, output_csv_name):\n",
        "    \n",
        "    directory = directory_string\n",
        "    class_list = os.listdir(directory) \n",
        "    class_list.sort() \n",
        "\n",
        "    \n",
        "    with open(output_csv_name, 'w', newline='') as csvfile:\n",
        "        \n",
        "        ### 실습 : CSV 파일 Object 생성 \n",
        "        writer = csv.writer(csvfile, delimiter=',')\n",
        "        ############################################\n",
        "        writer.writerow(['file_name', 'file_path', 'class_name', 'class_index']) # CSV의 column 이름을 지정\n",
        "\n",
        "        ###### 각 folder에 들어가서 각 이미지의 이름을 가져옴\n",
        "        for class_name in class_list:\n",
        "          class_path = os.path.join(directory, class_name)  \n",
        "          file_list = os.listdir(class_path) # 해당 파일 내부의 이미지를 확보\n",
        "          for file_name in file_list:\n",
        "              file_path = os.path.join(directory, class_name, file_name) #concatenate class folder dir, class name and file name\n",
        "              writer.writerow([file_name, file_path, class_name, class_name.split(\"_\")[1]]) #write the file path and class name to the csv file\n",
        "        #############################\n",
        "            \n",
        "    return\n",
        "\n",
        "train_folder = os.path.join(os.getcwd(), 'semicon')\n",
        "build_csv(train_folder, 'train.csv')\n",
        "train_df = pd.read_csv('train.csv')\n"
      ],
      "metadata": {
        "id": "5dXEooJlWsRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom데이터를 활용하기 위한 Dataset을 선언\n",
        "- pytorch의 `dataloader`를 이용하기 위해서는 `torch.utils.data.Dataset` 클래스를 상속한 클래스의 선언이 필요하다.\n",
        "- 클래스 내에 `__init__`, `__getitem__`, `__len__`의 3개의 메소드를 선언하여 오버라이드한다."
      ],
      "metadata": {
        "id": "i837jqmdjrGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class semiconductorDataset(Dataset): # inheritin from Dataset class\n",
        "\n",
        "    def __init__(self, csv_file, root_dir=\"\", transform=None):\n",
        "        self.annotation_df = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir # root directory of images, leave \"\" if using the image path column in the __getitem__ method\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotation_df) # return length (numer of rows) of the dataframe\n",
        "\n",
        "    ############ 실습 : annotation_df에 있는 이미지를 읽어 들어서, 변경한후\n",
        "    ############        읽어 들인 값을 return 하는 함수를 작성한다. \n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        ##### Image 읽기 \n",
        "        image_path = os.path.join(self.root_dir, self.annotation_df.iloc[idx, 1]) #use image path column (index = 1) in csv file\n",
        "        image = cv2.imread(image_path) # read image by cv2\n",
        "        #### 이미지를 Channel순서를 변경\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # convert from BGR to RGB for matplotlib\n",
        "        #### 이미지 Transform \n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        #### 이미지에 대한 추가 정보 확인 \n",
        "        class_name = self.annotation_df.iloc[idx, 2] # use class name column (index = 2) in csv file\n",
        "        class_index = self.annotation_df.iloc[idx, 3] # use class index column (index = 3) in csv file\n",
        "\n",
        "        return image, class_name, class_index"
      ],
      "metadata": {
        "id": "qOUmcMyjYn-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(), # PIL Image를 Tensor로 변경 \n",
        "    transforms.Resize((100,100)), # 크기 변경\n",
        "    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)), # Normalize\n",
        "    transforms.Grayscale(), # Gray Scale로 변경 \n",
        "    transforms.RandomAdjustSharpness(sharpness_factor=2), # randomly adjusts the sharpness\n",
        "    transforms.RandomAutocontrast() # 임의로 Contrast변경\n",
        "])\n",
        "\n",
        "##### 실습 : train_dataset 선언\n",
        "train_dataset = semiconductorDataset(csv_file='train.csv', root_dir=\"\", transform=transform)"
      ],
      "metadata": {
        "id": "aH1Zej7hLxxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batch 단위 학습을 위한 DataLoader 선언\n",
        " - Dataset내부의 샘플들을 batch 크기로 추출\n",
        " - Batch Size는 1step에 들어간 데이터의 개수\n",
        " - Epoch 마다 데이터를 섞어(Shuffle) Overfitting을 방지 \n",
        " - 병렬처리를 지원하여 데이터 검색 속도를 향상\n"
      ],
      "metadata": {
        "id": "m5pV5UK4kBiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### 실습 : torch.utils.DataLoader를 이용하여 데이터를 load. Batch 크기를 10으로 한다. \n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=10, shuffle=True, num_workers=2)\n",
        "\n",
        "for i, data in enumerate(train_dataloader):\n",
        "  images, class_name, labels = data\n",
        "  print(images.shape, labels.shape)\n",
        "\n",
        "  # 5번만 데이터를 load하고 멈춘다\n",
        "  if i > 3:\n",
        "    break"
      ],
      "metadata": {
        "id": "bCTnlT1pRpSS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f36cf199-cb71-42fc-93ee-8df5c1eeb872"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 1, 100, 100]) torch.Size([10])\n",
            "torch.Size([10, 1, 100, 100]) torch.Size([10])\n",
            "torch.Size([10, 1, 100, 100]) torch.Size([10])\n",
            "torch.Size([10, 1, 100, 100]) torch.Size([10])\n",
            "torch.Size([10, 1, 100, 100]) torch.Size([10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VGG6를 이용하여 학습을 수행한다\n",
        "![](https://drive.google.com/uc?export=view&id=1vLrvhxczx1ZCOH05cxzzESylaOaF1Uj2)\n"
      ],
      "metadata": {
        "id": "WhDfo_bXniHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class VGG_BatchNormTorch(nn.Module):\n",
        "\n",
        "  ### 실습 : 입력과 출력의 개수를 인자로 받을 수 있게 수정한다\n",
        "  def __init__(self, ???):\n",
        "    super(VGG_BatchNormTorch, self).__init__()\n",
        "\n",
        "    ###### 실습 : Gray이미지 이므로 in_channel =1 로 선언한다\n",
        "    self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=3, padding=1) \n",
        "    self.norm1 = torch.nn.BatchNorm2d(32)\n",
        "    self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n",
        "    self.norm2 = torch.nn.BatchNorm2d(32)\n",
        "    \n",
        "    self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "    self.norm3 = torch.nn.BatchNorm2d(64)\n",
        "    \n",
        "    self.conv4 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
        "    self.norm4 = torch.nn.BatchNorm2d(64)\n",
        "\n",
        "    self.conv5 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
        "    #### 실습 : Batch Norm 을 Pytorch의 값으로 변경한다 \n",
        "    self.norm5 = torch.nn.BatchNorm2d(128)\n",
        "    self.conv6 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n",
        "    #### 실습 : Batch Norm 을 Pytorch의 값으로 변경한다 \n",
        "    self.norm6 = torch.nn.BatchNorm2d(128)\n",
        "\n",
        "    self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2) #Maxpooling layer to change feature size \n",
        "    self.avg_pool = nn.AdaptiveAvgPool2d(output_size = (1, 1)) #Note that average pooling layer is not adopted in original VGG architecture. We use average pooling layer to make the architecture for experiment simple.\n",
        "\n",
        "    ###### 실습 : 11개의 Label이 있으므로 out_features=11로 선언한다\n",
        "    self.fc = nn.Linear(in_features=128, out_features=out_features)\n",
        "\n",
        "  def forward(self, x):\n",
        "    #### 실습 : Batch Norm을 Convolution 이후에 선언 \n",
        "    x = self.norm1(self.conv1(x))\n",
        "    x = F.relu(x)\n",
        "    x = self.norm2(self.conv2(x))\n",
        "    x = F.relu(x)\n",
        "    x = self.max_pool(x)\n",
        "\n",
        "    x = self.norm3(self.conv3(x))\n",
        "    x = F.relu(x)\n",
        "    x = self.norm4(self.conv4(x))\n",
        "    x = F.relu(x)\n",
        "    x = self.max_pool(x)\n",
        "\n",
        "    x = self.norm5(self.conv5(x))\n",
        "    x = F.relu(x)\n",
        "    x = self.norm6(self.conv6(x))\n",
        "    x = F.relu(x)\n",
        "\n",
        "    x = self.avg_pool(x)\n",
        "    x = x.view(-1, 128)\n",
        "    x = self.fc(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "QY8Iolz6gvHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, data_loader, criterion, optimizer, n_epoch):\n",
        "  \n",
        "  model.train() #\n",
        "  for epoch in range(n_epoch):\n",
        "    running_loss = 0\n",
        "    ### 실습 : Custom dataloader 에서 값을 가져온다\n",
        "    for i, (???) in enumerate(data_loader):\n",
        "      images, labels = images.cuda(), labels.cuda()\n",
        "      outputs = model(images)\n",
        "      loss = criterion(outputs, labels)\n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      running_loss += loss.item()\n",
        "      if (i + 1) % 100 == 0:\n",
        "        print('iteration: [{}/{}]'.format(i + 1, len(data_loader)))\n",
        "          \n",
        "    print('Epoch {}, loss = {:.3f}'.format(epoch + 1, running_loss/len(data_loader)))"
      ],
      "metadata": {
        "id": "yHya_ZR3vj0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval(model, data_loader):\n",
        "  \n",
        "  model.eval()\n",
        "  total = 0\n",
        "  correct = 0\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    ### 실습 : Custom dataloader 에서 값을 가져온다\n",
        "    for ???  labels in data_loader:\n",
        "      images, labels = images.cuda(), labels.cuda()\n",
        "      outputs = model(images)\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "      \n",
        "  print('Test Accuracy: {}%'.format(accuracy))"
      ],
      "metadata": {
        "id": "HjX476cmvhSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reset_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)"
      ],
      "metadata": {
        "id": "RLI00mauwDyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reset_seed(2020)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "##### 실습 : Gray 이미지를 받도록 한다. \n",
        "vgg_batchnorm_model = ???\n",
        "optimizer = optim.Adam(params=vgg_batchnorm_model.parameters())\n",
        "\n",
        "train(vgg_batchnorm_model, train_dataloader, criterion, optimizer, n_epoch=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t85o-4CZve4T",
        "outputId": "85c1efc3-9105-4fc1-e509-f8d623556f57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, loss = 2.448\n",
            "Epoch 2, loss = 2.346\n",
            "Epoch 3, loss = 2.228\n",
            "Epoch 4, loss = 2.179\n",
            "Epoch 5, loss = 2.107\n",
            "Epoch 6, loss = 1.972\n",
            "Epoch 7, loss = 1.802\n",
            "Epoch 8, loss = 1.752\n",
            "Epoch 9, loss = 1.685\n",
            "Epoch 10, loss = 1.573\n",
            "Epoch 11, loss = 1.447\n",
            "Epoch 12, loss = 1.382\n",
            "Epoch 13, loss = 1.384\n",
            "Epoch 14, loss = 1.292\n",
            "Epoch 15, loss = 1.227\n",
            "Epoch 16, loss = 1.198\n",
            "Epoch 17, loss = 1.151\n",
            "Epoch 18, loss = 1.094\n",
            "Epoch 19, loss = 1.075\n",
            "Epoch 20, loss = 1.008\n",
            "Epoch 21, loss = 0.940\n",
            "Epoch 22, loss = 1.003\n",
            "Epoch 23, loss = 0.983\n",
            "Epoch 24, loss = 0.914\n",
            "Epoch 25, loss = 0.889\n",
            "Epoch 26, loss = 0.852\n",
            "Epoch 27, loss = 0.830\n",
            "Epoch 28, loss = 0.785\n",
            "Epoch 29, loss = 0.681\n",
            "Epoch 30, loss = 0.698\n",
            "Epoch 31, loss = 0.690\n",
            "Epoch 32, loss = 0.590\n",
            "Epoch 33, loss = 0.622\n",
            "Epoch 34, loss = 0.554\n",
            "Epoch 35, loss = 0.497\n",
            "Epoch 36, loss = 0.431\n",
            "Epoch 37, loss = 0.371\n",
            "Epoch 38, loss = 0.398\n",
            "Epoch 39, loss = 0.375\n",
            "Epoch 40, loss = 0.352\n",
            "Epoch 41, loss = 0.341\n",
            "Epoch 42, loss = 0.397\n",
            "Epoch 43, loss = 0.363\n",
            "Epoch 44, loss = 0.368\n",
            "Epoch 45, loss = 0.322\n",
            "Epoch 46, loss = 0.280\n",
            "Epoch 47, loss = 0.265\n",
            "Epoch 48, loss = 0.245\n",
            "Epoch 49, loss = 0.232\n",
            "Epoch 50, loss = 0.216\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 정확도를 평가해 봅시다 \n",
        "* 데이터가 너무 적어서 평가 데이터에 대한 정확도를 봅니다"
      ],
      "metadata": {
        "id": "8d4AUYGHJzQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval(vgg_batchnorm_model, train_dataloader)"
      ],
      "metadata": {
        "id": "cdj0G1XrrBkv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eaf4494e-edef-4416-f68d-cd5de6e98c38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 91.91919191919192%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RGB로 학습 후 평가해 본다"
      ],
      "metadata": {
        "id": "qJ-2OryDzP4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "#### Gray Scale을 제거한 Transform 을 구성한다\n",
        "transform = transforms.Compose([\n",
        "???\n",
        "])\n",
        "\n",
        "train_dataset = semiconductorDataset(csv_file='train.csv', root_dir=\"\", transform=transform)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=10, shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "id": "k-skeZ8GzPQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reset_seed(2020)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "############### 실습 : 입력 Channel의 개수는 3\n",
        "vgg_batchnorm_model = ???\n",
        "optimizer = optim.Adam(params=vgg_batchnorm_model.parameters())\n",
        "\n",
        "train(vgg_batchnorm_model, train_dataloader, criterion, optimizer, n_epoch=40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4PSMYpRzQ0B",
        "outputId": "689b146d-8b55-432f-9e01-784d52344d58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, loss = 2.386\n",
            "Epoch 2, loss = 2.198\n",
            "Epoch 3, loss = 2.092\n",
            "Epoch 4, loss = 2.011\n",
            "Epoch 5, loss = 2.011\n",
            "Epoch 6, loss = 2.035\n",
            "Epoch 7, loss = 1.980\n",
            "Epoch 8, loss = 1.916\n",
            "Epoch 9, loss = 1.896\n",
            "Epoch 10, loss = 1.836\n",
            "Epoch 11, loss = 1.739\n",
            "Epoch 12, loss = 1.600\n",
            "Epoch 13, loss = 1.549\n",
            "Epoch 14, loss = 1.492\n",
            "Epoch 15, loss = 1.489\n",
            "Epoch 16, loss = 1.464\n",
            "Epoch 17, loss = 1.415\n",
            "Epoch 18, loss = 1.348\n",
            "Epoch 19, loss = 1.204\n",
            "Epoch 20, loss = 1.208\n",
            "Epoch 21, loss = 1.194\n",
            "Epoch 22, loss = 1.219\n",
            "Epoch 23, loss = 1.158\n",
            "Epoch 24, loss = 1.102\n",
            "Epoch 25, loss = 1.144\n",
            "Epoch 26, loss = 1.052\n",
            "Epoch 27, loss = 0.924\n",
            "Epoch 28, loss = 0.858\n",
            "Epoch 29, loss = 0.858\n",
            "Epoch 30, loss = 0.822\n",
            "Epoch 31, loss = 0.776\n",
            "Epoch 32, loss = 0.815\n",
            "Epoch 33, loss = 0.743\n",
            "Epoch 34, loss = 0.800\n",
            "Epoch 35, loss = 0.843\n",
            "Epoch 36, loss = 0.659\n",
            "Epoch 37, loss = 0.651\n",
            "Epoch 38, loss = 0.583\n",
            "Epoch 39, loss = 0.627\n",
            "Epoch 40, loss = 0.599\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval(vgg_batchnorm_model, train_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HtIt4Va0KmL",
        "outputId": "2a21c1e5-8742-4d16-b2f7-72028826d25e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 80.3030303030303%\n"
          ]
        }
      ]
    }
  ]
}