{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP74v8uLQz5mTCHrsDsbsU/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chminPark/ml-python/blob/master/%EC%8B%A4%EC%8A%B5_CNN_EarlyStopping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "import copy\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import os\n",
        "\n",
        "torch.backends.cudnn.deterministic = True # Use cudnn as deterministic mode for reproducibility\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "fvM0McnQ8Mx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VGG 모델 생성"
      ],
      "metadata": {
        "id": "3qFdJimL3oT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(VGG, self).__init__()\n",
        "\n",
        "      self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1) # Convolutional layer with 3x3 kernel. The size of feature does not change due to the usage of padding.\n",
        "      self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n",
        "\n",
        "      self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "      self.conv4 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
        "\n",
        "      self.conv5 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
        "      self.conv6 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n",
        "\n",
        "      self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2) #Maxpooling layer to change feature size\n",
        "      self.avg_pool = nn.AdaptiveAvgPool2d(output_size = (1, 1)) ## 실습: 이미지의 평균값을 계산하는 Layer\n",
        "      self.fc = nn.Linear(in_features=128, out_features=10) # 실습 : 10개 Class를 만들어 내는 FC\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.conv1(x)\n",
        "      x = F.relu(x)\n",
        "      x = self.conv2(x)\n",
        "      x = F.relu(x)\n",
        "      x = self.max_pool(x)\n",
        "\n",
        "      x = self.conv3(x)\n",
        "      x = F.relu(x)\n",
        "      x = self.conv4(x)\n",
        "      x = F.relu(x)\n",
        "      x = self.max_pool(x)\n",
        "\n",
        "      x = self.conv5(x)\n",
        "      x = F.relu(x)\n",
        "      x = self.conv6(x)\n",
        "      x = F.relu(x)   # 128x8x8\n",
        "      x = self.avg_pool(x) # 128x1x1\n",
        "\n",
        "      x = x.view(-1, 128)\n",
        "      x = self.fc(x)\n",
        "      return x"
      ],
      "metadata": {
        "id": "iUMYk_m33n62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CIFAR-10 데이터를 이용한다."
      ],
      "metadata": {
        "id": "oR6SrnSXNdbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4), # Random Crop: Randomly crop the part of the large image and utilize it as an augmented data\n",
        "        transforms.RandomHorizontalFlip(), # Random Horizontal Flip: Randomly flip the image and utilize it as an augmented data\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023,0.1994,0.2010]), # Normalize the data using the given mean and standard deviation\n",
        "        ])\n",
        "\n",
        "#Apply data preprocessing for test set\n",
        "transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023,0.1994,0.2010]),\n",
        "        ])\n",
        "\n",
        "dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5r2VhcQPJwl",
        "outputId": "6bdadedc-9958-4a6e-c92a-c6efc9553479"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBywEPi5xATJ",
        "outputId": "f27a667e-c514-4c41-dcea-b545317327ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['airplane',\n",
              " 'automobile',\n",
              " 'bird',\n",
              " 'cat',\n",
              " 'deer',\n",
              " 'dog',\n",
              " 'frog',\n",
              " 'horse',\n",
              " 'ship',\n",
              " 'truck']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_count = {}\n",
        "for _, index in dataset:\n",
        "    label = dataset.classes[index]\n",
        "    if label not in class_count:\n",
        "        class_count[label] = 0\n",
        "    class_count[label] += 1\n",
        "class_count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJQPhHbvyyhT",
        "outputId": "7fcbebc1-52a4-4914-850f-785ddf6067c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'frog': 5000,\n",
              " 'truck': 5000,\n",
              " 'deer': 5000,\n",
              " 'automobile': 5000,\n",
              " 'bird': 5000,\n",
              " 'horse': 5000,\n",
              " 'ship': 5000,\n",
              " 'cat': 5000,\n",
              " 'dog': 5000,\n",
              " 'airplane': 5000}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train과 Validation 데이터를 나눈다"
      ],
      "metadata": {
        "id": "U72QwsK9zfLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset을 train 과 validation데이터로 나눈다.\n",
        "# Pytorch DataSet 을 나눌때에는 random_split을 수행한다.\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "train_len = int(len(dataset) * 0.8)\n",
        "val_len = len(dataset) - train_len\n",
        "\n",
        "train_dataset, val_dataset = random_split(dataset, [train_len, val_len])\n",
        "len(train_dataset), len(val_dataset)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfNUtbCKxDxB",
        "outputId": "9573b687-fb7c-4042-f7f5-77b13bca46ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40000, 10000)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=100, shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100)"
      ],
      "metadata": {
        "id": "eFsg3Mtaw9F-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Early Stopping 객체를 생성한다"
      ],
      "metadata": {
        "id": "osEXE_OjzzQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "            path (str): Path for the checkpoint to be saved to.\n",
        "                            Default: 'checkpoint.pt'\n",
        "            trace_func (function): trace print function.\n",
        "                            Default: print\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.trace_func = trace_func\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss"
      ],
      "metadata": {
        "id": "SMGfZMnhzyyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 학습함수를 선언하고 학습을 수행한다"
      ],
      "metadata": {
        "id": "gpA1CP5kZ-nD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reset_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)"
      ],
      "metadata": {
        "id": "Z5Ol1L6jzmw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, data_loader, val_loader, early_stopping, criterion, optimizer, n_epoch):\n",
        "  ### 실습 : 학습 중임을 알림\n",
        "  model.train() ###\n",
        "  for epoch in range(n_epoch):\n",
        "    running_loss = 0\n",
        "    for i, (images, labels) in enumerate(data_loader):\n",
        "\n",
        "      images, labels = images.cuda(), labels.cuda()\n",
        "      outputs = model(images)\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      running_loss += loss.item()\n",
        "      if (i + 1) % 100 == 0:\n",
        "        print('iteration: [{}/{}]'.format(i + 1, len(data_loader)))\n",
        "\n",
        "    # 각 epoch 이후 validation 수행\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    for images, labels in val_loader:\n",
        "      images, labels = images.cuda(), labels.cuda()\n",
        "      outputs = model(images)\n",
        "      vloss = criterion(outputs, labels)\n",
        "\n",
        "      val_loss += vloss\n",
        "\n",
        "    # early stopping을 체크\n",
        "    early_stopping(val_loss, model)\n",
        "\n",
        "    # overfitting 발생 시 학습 종료\n",
        "    if early_stopping.early_stop:\n",
        "      break\n",
        "\n",
        "    # 원래 상태로 돌아옴\n",
        "    model.train()\n",
        "    running_loss = running_loss/len(data_loader)\n",
        "    val_loss = val_loss / len(val_loader)\n",
        "    print(f'Epoch {epoch + 1}, train_loss = {running_loss:.3f}, val_loss = {val_loss:.3f}')"
      ],
      "metadata": {
        "id": "XdzlE2_OUz33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 학습수행"
      ],
      "metadata": {
        "id": "GJ-2pqrT3lIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reset_seed(10000) # seed 값에 따라서 딥러닝모델의 weight가 초기화 됩니다.\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "vgg_model = VGG().to(\"cuda\")\n",
        "optimizer = optim.Adam(params=vgg_model.parameters())\n",
        "\n",
        "# Early Stop 객체를 생성하고 validation loss가 연속으로 3번 상승되면 정지하는 코드를 만든다.\n",
        "earlystop = EarlyStopping(patience=3)\n",
        "train(vgg_model, train_loader, val_loader, earlystop, criterion, optimizer, n_epoch=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DV3Zjb9BVo15",
        "outputId": "627416fc-fa4e-4192-a8a2-1022ec035167"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration: [100/400]\n",
            "iteration: [200/400]\n",
            "iteration: [300/400]\n",
            "iteration: [400/400]\n",
            "Epoch 1, train_loss = 1.836, val_loss = 1.571\n",
            "iteration: [100/400]\n",
            "iteration: [200/400]\n",
            "iteration: [300/400]\n",
            "iteration: [400/400]\n",
            "Epoch 2, train_loss = 1.425, val_loss = 1.348\n",
            "iteration: [100/400]\n",
            "iteration: [200/400]\n",
            "iteration: [300/400]\n",
            "iteration: [400/400]\n",
            "Epoch 3, train_loss = 1.203, val_loss = 1.117\n",
            "iteration: [100/400]\n",
            "iteration: [200/400]\n",
            "iteration: [300/400]\n",
            "iteration: [400/400]\n",
            "Epoch 4, train_loss = 1.076, val_loss = 1.023\n",
            "iteration: [100/400]\n",
            "iteration: [200/400]\n",
            "iteration: [300/400]\n",
            "iteration: [400/400]\n",
            "Epoch 5, train_loss = 0.986, val_loss = 0.930\n",
            "iteration: [100/400]\n",
            "iteration: [200/400]\n",
            "iteration: [300/400]\n",
            "iteration: [400/400]\n",
            "Epoch 6, train_loss = 0.914, val_loss = 0.905\n",
            "iteration: [100/400]\n",
            "iteration: [200/400]\n",
            "iteration: [300/400]\n",
            "iteration: [400/400]\n",
            "Epoch 7, train_loss = 0.858, val_loss = 0.875\n",
            "iteration: [100/400]\n",
            "iteration: [200/400]\n",
            "iteration: [300/400]\n",
            "iteration: [400/400]\n",
            "Epoch 8, train_loss = 0.805, val_loss = 0.825\n",
            "iteration: [100/400]\n",
            "iteration: [200/400]\n",
            "iteration: [300/400]\n",
            "iteration: [400/400]\n",
            "Epoch 9, train_loss = 0.761, val_loss = 0.802\n",
            "iteration: [100/400]\n",
            "iteration: [200/400]\n",
            "iteration: [300/400]\n",
            "iteration: [400/400]\n",
            "Epoch 10, train_loss = 0.712, val_loss = 0.741\n",
            "iteration: [100/400]\n",
            "iteration: [200/400]\n",
            "iteration: [300/400]\n",
            "iteration: [400/400]\n",
            "Epoch 11, train_loss = 0.688, val_loss = 0.709\n",
            "iteration: [100/400]\n",
            "iteration: [200/400]\n",
            "iteration: [300/400]\n",
            "iteration: [400/400]\n",
            "Epoch 12, train_loss = 0.654, val_loss = 0.647\n",
            "iteration: [100/400]\n",
            "iteration: [200/400]\n",
            "iteration: [300/400]\n",
            "iteration: [400/400]\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Epoch 13, train_loss = 0.622, val_loss = 0.666\n",
            "iteration: [100/400]\n",
            "iteration: [200/400]\n",
            "iteration: [300/400]\n",
            "iteration: [400/400]\n",
            "EarlyStopping counter: 2 out of 3\n",
            "Epoch 14, train_loss = 0.597, val_loss = 0.650\n",
            "iteration: [100/400]\n",
            "iteration: [200/400]\n",
            "iteration: [300/400]\n",
            "iteration: [400/400]\n",
            "Epoch 15, train_loss = 0.579, val_loss = 0.602\n",
            "iteration: [100/400]\n",
            "iteration: [200/400]\n",
            "iteration: [300/400]\n",
            "iteration: [400/400]\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Epoch 16, train_loss = 0.557, val_loss = 0.617\n",
            "iteration: [100/400]\n",
            "iteration: [200/400]\n",
            "iteration: [300/400]\n",
            "iteration: [400/400]\n",
            "Epoch 17, train_loss = 0.535, val_loss = 0.576\n",
            "iteration: [100/400]\n",
            "iteration: [200/400]\n",
            "iteration: [300/400]\n",
            "iteration: [400/400]\n",
            "Epoch 18, train_loss = 0.512, val_loss = 0.574\n",
            "iteration: [100/400]\n",
            "iteration: [200/400]\n",
            "iteration: [300/400]\n",
            "iteration: [400/400]\n",
            "Epoch 19, train_loss = 0.504, val_loss = 0.564\n",
            "iteration: [100/400]\n",
            "iteration: [200/400]\n",
            "iteration: [300/400]\n",
            "iteration: [400/400]\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Epoch 20, train_loss = 0.489, val_loss = 0.600\n",
            "iteration: [100/400]\n",
            "iteration: [200/400]\n",
            "iteration: [300/400]\n",
            "iteration: [400/400]\n",
            "Epoch 21, train_loss = 0.471, val_loss = 0.563\n",
            "iteration: [100/400]\n",
            "iteration: [200/400]\n",
            "iteration: [300/400]\n",
            "iteration: [400/400]\n",
            "Epoch 22, train_loss = 0.459, val_loss = 0.527\n",
            "iteration: [100/400]\n",
            "iteration: [200/400]\n",
            "iteration: [300/400]\n",
            "iteration: [400/400]\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Epoch 23, train_loss = 0.446, val_loss = 0.561\n",
            "iteration: [100/400]\n",
            "iteration: [200/400]\n",
            "iteration: [300/400]\n",
            "iteration: [400/400]\n",
            "EarlyStopping counter: 2 out of 3\n",
            "Epoch 24, train_loss = 0.433, val_loss = 0.600\n",
            "iteration: [100/400]\n",
            "iteration: [200/400]\n",
            "iteration: [300/400]\n",
            "iteration: [400/400]\n",
            "Epoch 25, train_loss = 0.421, val_loss = 0.507\n",
            "iteration: [100/400]\n",
            "iteration: [200/400]\n",
            "iteration: [300/400]\n",
            "iteration: [400/400]\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Epoch 26, train_loss = 0.408, val_loss = 0.514\n",
            "iteration: [100/400]\n",
            "iteration: [200/400]\n",
            "iteration: [300/400]\n",
            "iteration: [400/400]\n",
            "EarlyStopping counter: 2 out of 3\n",
            "Epoch 27, train_loss = 0.398, val_loss = 0.530\n",
            "iteration: [100/400]\n",
            "iteration: [200/400]\n",
            "iteration: [300/400]\n",
            "iteration: [400/400]\n",
            "EarlyStopping counter: 3 out of 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 평가 함수를 구성하고 평가 정확도를 확인한다"
      ],
      "metadata": {
        "id": "lGaoRR1iZ5Nt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# early stop 된 객체를 load 한다.\n",
        "state_dict = torch.load('checkpoint.pt')\n",
        "vgg_model = VGG().to(\"cuda\")\n",
        "vgg_model.load_state_dict(state_dict)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhvsDdoB_NWu",
        "outputId": "22f16540-357e-44c1-8d73-bd6b1958f589"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def eval(model, data_loader):\n",
        "  #### 실습 : 평가를 위해서는 eval()을 선언\n",
        "  model.eval()\n",
        "  total = 0\n",
        "  correct = 0\n",
        "  #### 실습 : Gradient를 타지 않아야 한다\n",
        "  with torch.no_grad():\n",
        "    for images, labels in data_loader:\n",
        "      images, labels = images.cuda(), labels.cuda()\n",
        "      outputs = model(images)\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "  print('Test Accuracy: {}%'.format(accuracy))"
      ],
      "metadata": {
        "id": "1aX7lDqDWPGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval(vgg_model, test_loader)"
      ],
      "metadata": {
        "id": "rwbjZYTFZ3ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cf796f7-595a-419f-d0e3-ac37e66dec2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 82.34%\n"
          ]
        }
      ]
    }
  ]
}